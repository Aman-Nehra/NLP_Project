{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ab12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daab6d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "0               neutral                @VirginAmerica What @dhepburn said.\n",
       "1              positive  @VirginAmerica plus you've added commercials t...\n",
       "2               neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3              negative  @VirginAmerica it's really aggressive to blast...\n",
       "4              negative  @VirginAmerica and it's a really big bad thing...\n",
       "...                 ...                                                ...\n",
       "14635          positive  @AmericanAir thank you we got on a different f...\n",
       "14636          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14637           neutral  @AmericanAir Please bring American Airlines to...\n",
       "14638          negative  @AmericanAir you have my money, you change my ...\n",
       "14639           neutral  @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Tweets.csv',usecols=['airline_sentiment','text'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00db7e6",
   "metadata": {},
   "source": [
    "#### PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192bcc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['plus',\n",
       " 'you',\n",
       " 'have',\n",
       " 'added',\n",
       " 'commercial',\n",
       " 'to',\n",
       " 'the',\n",
       " 'experience',\n",
       " 'tacky']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contraction_map = {\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"ain't\": \"is not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(tweet):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in contraction_map.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contraction_map[x.group()],tweet)\n",
    "\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+','',tweet)\n",
    "    # Remove mentions\n",
    "    tweet = re.sub(r'@\\w+','',tweet)\n",
    "    # Remove Hashtags\n",
    "    tweet = re.sub(r'#\\w*','',tweet)\n",
    "    # Expanding contractions\n",
    "    tweet = expand_contractions(tweet)\n",
    "    # Removing special characters\n",
    "    tweet = re.sub(r'[^\\w\\s]','',tweet) # Removes everything expect alphanumerics and spaces\n",
    "    # Removing Emojis\n",
    "    tweet = re.sub(r'[\\U00010000-\\U0010ffff]','',tweet)\n",
    "    # tokenize\n",
    "    words = tweet.split()\n",
    "    tweet = [word for word in words if word not in string.punctuation]\n",
    "    # Lemmatization\n",
    "    Lemmatizer = WordNetLemmatizer()\n",
    "    tweet = [Lemmatizer.lemmatize(word) for word in tweet]\n",
    "\n",
    "    return tweet\n",
    "\n",
    "print(data.iloc[1,1])\n",
    "preprocess(data.iloc[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464d297",
   "metadata": {},
   "source": [
    "#### Loading Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bcf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8003ce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Vector ===\n",
      "\n",
      "[[ 0.0652771  -0.025177    0.15722656 ...  0.10083008  0.14013672\n",
      "  -0.16064453]\n",
      " [ 0.02523804 -0.00747681 -0.0210495  ... -0.10122681 -0.00774956\n",
      "  -0.02825165]\n",
      " [-0.01674028  0.03279252  0.05932617 ... -0.07518422  0.01525879\n",
      "  -0.07044566]\n",
      " ...\n",
      " [-0.01654053 -0.00634766  0.08666992 ...  0.00134277  0.0660553\n",
      "  -0.15252686]\n",
      " [ 0.01665982 -0.01029275  0.04662392 ... -0.07168579 -0.01131439\n",
      "  -0.03199317]\n",
      " [ 0.03410181  0.00179948  0.04938612 ... -0.04442938  0.02539852\n",
      "  -0.04221081]]\n",
      "\n",
      "=== Labels ===\n",
      "\n",
      "0         neutral\n",
      "1        positive\n",
      "2         neutral\n",
      "3        negative\n",
      "4        negative\n",
      "           ...   \n",
      "14635    positive\n",
      "14636    negative\n",
      "14637     neutral\n",
      "14638    negative\n",
      "14639     neutral\n",
      "Name: airline_sentiment, Length: 14640, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def avgWord2Vec(tweet,model):\n",
    "    valid_vectors = [model[word] for word in tweet if word in model]\n",
    "\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(valid_vectors,axis=0)\n",
    "    \n",
    "tweets = []\n",
    "for i in range(data.shape[0]):\n",
    "    tweets.append(preprocess(data.iloc[i,1]))\n",
    "\n",
    "X = np.zeros((data.shape[0],300))\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    vector = avgWord2Vec(tweets[i],w2v_model)\n",
    "    X[i] = vector\n",
    "\n",
    "Y = data['airline_sentiment']\n",
    "print('\\n=== Feature Vector ===\\n')\n",
    "print(X)\n",
    "print('\\n=== Labels ===\\n')\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fcac5",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b825c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b880006",
   "metadata": {},
   "source": [
    "#### Training & Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c35ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Model :  0.7858606557377049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier_model = LogisticRegression()\n",
    "classifier_model.fit(X_train,Y_train)\n",
    "Y_Pred = classifier_model.predict(X_test)\n",
    "print(\"Accuracy of the Model : \",accuracy_score(Y_test,Y_Pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c881b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet_class(model,w2v_model,tweet):\n",
    "    tweet = preprocess(tweet)\n",
    "    tweet_vector = avgWord2Vec(tweet,w2v_model).reshape(1,-1)\n",
    "    return model.predict(tweet_vector)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aa4ed",
   "metadata": {},
   "source": [
    "#### Testing on new Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d99ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1 = \"Just landed my dream job! So grateful for this opportunity. ðŸ’¼âœ¨ #blessed #careergoals\"\n",
    "predict_tweet_class(classifier_model,w2v_model,tweet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41bdab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2 = \"Customer service was absolutely terrible. Wasted an hour and got no help. ðŸ˜¡ #disappointed\"\n",
    "predict_tweet_class(classifier_model,w2v_model,tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2936a3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet3 = \"Today I arrived at New York City !!\"\n",
    "predict_tweet_class(classifier_model,w2v_model,tweet3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
