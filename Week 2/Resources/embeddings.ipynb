{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6676254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\amann\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30276d1a",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "709bc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "163634f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34e365b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
      " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
      "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
      " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
      "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
      "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
      "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
      "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
      "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
      "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
      "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
      "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
      " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
      " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
      " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
      "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
      "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
      " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
      " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
      " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
      "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
      "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
      "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
      " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
      " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
      "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
      " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
      "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
      " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
      " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
      "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
      " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
      " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
      "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
      " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
      "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
      "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
      " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
      " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
      " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
      " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
      " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
      "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
      "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
      "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
      " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
      "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
      "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
      " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
      " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
      " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
      "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
      "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
      " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
      "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
      " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
      "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
      "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
      " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
      "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
      "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
      "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
      "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
      "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
      " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
      "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
      " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
      "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
      "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
      " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
      "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
      "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
      "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
      " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
      " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
      "[('kings', 0.7138044834136963), ('queen', 0.6510957479476929), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204219460487366), ('prince', 0.6159994602203369), ('sultan', 0.5864822864532471), ('ruler', 0.5797566175460815), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422106385231018)]\n"
     ]
    }
   ],
   "source": [
    "print(model['king']) # see vector\n",
    "print(model.most_similar('king')) # analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1775044b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'dog' : \n",
      "[('dogs', 0.8680490851402283), ('puppy', 0.8106428384780884), ('pit_bull', 0.780396044254303), ('pooch', 0.7627375721931458), ('cat', 0.7609457969665527), ('golden_retriever', 0.7500901222229004), ('German_shepherd', 0.7465173006057739), ('Rottweiler', 0.7437615990638733), ('beagle', 0.7418619990348816), ('pup', 0.7406911253929138)]\n",
      "Word analogy: king - man + woman = ?\n",
      "[('queen', 0.7118191123008728)]\n",
      "Similarity between coffee and tea :\n",
      "0.56352925\n",
      "Is 'dragon' in the vocabulary ?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "print(\"Most similar to 'dog' : \")\n",
    "print(model.most_similar('dog'))\n",
    "\n",
    "# Word analogy: king - man + woman = ?? ---> queen\n",
    "print(\"Word analogy: king - man + woman = ?\")\n",
    "print(model.most_similar(positive=['king','woman'],negative=['man'],topn=1))\n",
    "\n",
    "# Word Similarity\n",
    "print(\"Similarity between coffee and tea :\")\n",
    "print(model.similarity('coffee','tea'))\n",
    "\n",
    "# Check if a word exists in vocabulary\n",
    "print(\"Is 'dragon' in the vocabulary ?\")\n",
    "print('dragon' in model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8d55c",
   "metadata": {},
   "source": [
    "#### Sentence Embeddings (Avg Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcfe7409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Vector (shape): (300,)\n",
      "First 5 dimensions: [ 0.01123047 -0.01138306  0.02069092  0.14361572 -0.03967285]\n"
     ]
    }
   ],
   "source": [
    "def avg_word2vec(sentence,model):\n",
    "    words = sentence.lower().split()\n",
    "    valid_vectors = [model[word] for word in words if word in model]\n",
    "\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(valid_vectors,axis=0)\n",
    "\n",
    "sentence = \"I love machine learning\"\n",
    "vector = avg_word2vec(sentence,model)\n",
    "\n",
    "print(\"Sentence Vector (shape):\", vector.shape)\n",
    "print(\"First 5 dimensions:\", vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88cf21",
   "metadata": {},
   "source": [
    "### Traditional Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be592414",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b3d956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'the', 'hat', 'in', 'tree', 'dog', 'on', 'cat', 'mat', 'bird'}\n",
      "Word to Index Mapping: {'the': 0, 'hat': 1, 'in': 2, 'tree': 3, 'dog': 4, 'on': 5, 'cat': 6, 'mat': 7, 'bird': 8}\n",
      "One-Hot Encoded Matrix:\n",
      "cat: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "in: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "the: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "hat: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "dog: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "on: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "the: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "mat: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "bird: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "in: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "the: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tree: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(text):\n",
    "    words = text.split()\n",
    "    vocabulary = set(words)\n",
    "    word_to_index = {word : i for i,word in enumerate(vocabulary)}\n",
    "    one_hot_encoded = []\n",
    "    for word in words:\n",
    "        one_hot_vector = [0] * len(vocabulary)\n",
    "        one_hot_vector[word_to_index[word]] = 1\n",
    "        one_hot_encoded.append(one_hot_vector)\n",
    "    return one_hot_encoded, word_to_index, vocabulary\n",
    "\n",
    "example_text = \"cat in the hat dog on the mat bird in the tree\"\n",
    "\n",
    "one_hot_encoded,word_to_index,vocabulary = one_hot_encode(example_text)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Word to Index Mapping:\", word_to_index)\n",
    "print(\"One-Hot Encoded Matrix:\")\n",
    "for word, encoding in zip(example_text.split(), one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7b372",
   "metadata": {},
   "source": [
    "#### Bag of Word (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d56a59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\"This is the first document.\", \"This document is the second document.\",\n",
    "              \"And this is the third one.\", \"Is this the first document?\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ab980",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6178108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: \n",
      "the: 0.6030226891555273\n",
      "quick: 0.30151134457776363\n",
      "brown: 0.30151134457776363\n",
      "fox: 0.30151134457776363\n",
      "jumps: 0.30151134457776363\n",
      "over: 0.30151134457776363\n",
      "lazy: 0.30151134457776363\n",
      "dog: 0.30151134457776363\n",
      "\n",
      "\n",
      "Document 2: \n",
      "journey: 0.3535533905932738\n",
      "of: 0.3535533905932738\n",
      "thousand: 0.3535533905932738\n",
      "miles: 0.3535533905932738\n",
      "begins: 0.3535533905932738\n",
      "with: 0.3535533905932738\n",
      "single: 0.3535533905932738\n",
      "step: 0.3535533905932738\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [ \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\" ]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = {}\n",
    "\n",
    "for doc_index,doc in enumerate(documents):\n",
    "    feature_index = tfidf_matrix[doc_index,:].nonzero()[1]\n",
    "    tfidf_doc_values = zip(feature_index,[tfidf_matrix[doc_index,x] for x in feature_index])\n",
    "    tfidf_values[doc_index] = {feature_names[i]: value for i,value in tfidf_doc_values}\n",
    "\n",
    "for doc_index,values in tfidf_values.items():\n",
    "    print(f\"Document {doc_index+1}: \")\n",
    "    for word,tfidf_value in values.items():\n",
    "        print(f\"{word}: {tfidf_value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50874b47",
   "metadata": {},
   "source": [
    "### Neural Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9d1eb",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35625e79",
   "metadata": {},
   "source": [
    "##### 1. Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c61633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'embeddings': [[-1.711798   -0.8385683   0.3857726  -0.51692265 -0.5410009   0.1587451\n",
      "  -0.3844856   1.8475201  -0.8461097   0.7197072 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define CBOW model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        context_embeds = self.embeddings(context).sum(dim=1)\n",
    "        output = self.linear(context_embeds)\n",
    "        return output\n",
    "\n",
    "context_size = 2\n",
    "raw_text = \"word embeddings are awesome\"\n",
    "tokens = raw_text.split()\n",
    "vocab = set(tokens)\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(tokens) - 2):\n",
    "    context = [word_to_index[word] for word in tokens[i - 2:i] + tokens[i + 1:i + 3]]\n",
    "    target = word_to_index[tokens[i]]\n",
    "    data.append((torch.tensor(context), torch.tensor(target)))\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Initialize CBOW model\n",
    "cbow_model = CBOWModel(vocab_size, embed_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = cbow_model(context)\n",
    "        loss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "# Example usage\n",
    "word_to_lookup = \"embeddings\"\n",
    "word_index = word_to_index[word_to_lookup]\n",
    "embedding = cbow_model.embeddings(torch.tensor([word_index]))\n",
    "print(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6daea82",
   "metadata": {},
   "source": [
    "##### 2. Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "183cfef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of 'word': [-9.5800208e-03  8.9437785e-03  4.1664648e-03  9.2367809e-03\n",
      "  6.6457358e-03  2.9233587e-03  9.8055992e-03 -4.4231843e-03\n",
      " -6.8048164e-03  4.2256550e-03  3.7299085e-03 -5.6668529e-03\n",
      "  9.7035142e-03 -3.5551414e-03  9.5499391e-03  8.3657773e-04\n",
      " -6.3355025e-03 -1.9741615e-03 -7.3781307e-03 -2.9811086e-03\n",
      "  1.0425397e-03  9.4814906e-03  9.3598543e-03 -6.5986011e-03\n",
      "  3.4773252e-03  2.2767992e-03 -2.4910474e-03 -9.2290826e-03\n",
      "  1.0267317e-03 -8.1645092e-03  6.3240929e-03 -5.8001447e-03\n",
      "  5.5353874e-03  9.8330071e-03 -1.5987856e-04  4.5296676e-03\n",
      " -1.8086446e-03  7.3613892e-03  3.9419360e-03 -9.0095028e-03\n",
      " -2.3953868e-03  3.6261671e-03 -1.0080514e-04 -1.2024897e-03\n",
      " -1.0558038e-03 -1.6681013e-03  6.0541567e-04  4.1633579e-03\n",
      " -4.2531900e-03 -3.8336846e-03 -5.0755290e-05  2.6549282e-04\n",
      " -1.7014991e-04 -4.7843382e-03  4.3120929e-03 -2.1710952e-03\n",
      "  2.1056964e-03  6.6702347e-04  5.9686624e-03 -6.8418151e-03\n",
      " -6.8183104e-03 -4.4762432e-03  9.4359247e-03 -1.5930856e-03\n",
      " -9.4291316e-03 -5.4270827e-04 -4.4478951e-03  5.9980620e-03\n",
      " -9.5831212e-03  2.8602476e-03 -9.2544509e-03  1.2484600e-03\n",
      "  6.0004774e-03  7.4001122e-03 -7.6209377e-03 -6.0561695e-03\n",
      " -6.8399287e-03 -7.9184016e-03 -9.4984965e-03 -2.1255787e-03\n",
      " -8.3757477e-04 -7.2564054e-03  6.7876028e-03  1.1183097e-03\n",
      "  5.8291717e-03  1.4714618e-03  7.9081533e-04 -7.3718326e-03\n",
      " -2.1769912e-03  4.3199472e-03 -5.0856168e-03  1.1304744e-03\n",
      "  2.8835384e-03 -1.5386029e-03  9.9318363e-03  8.3507905e-03\n",
      "  2.4184163e-03  7.1170190e-03  5.8888551e-03 -5.5787875e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "sample = \"Word embeddings are dense vector representations of words.\"\n",
    "\n",
    "tokenized_corpus = word_tokenize(sample.lower())\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=[tokenized_corpus],\n",
    "                          vector_size=100,\n",
    "                          window=5,\n",
    "                          sg=1,\n",
    "                          min_count=1,\n",
    "                          workers=4)\n",
    "\n",
    "# Training \n",
    "skipgram_model.train([tokenized_corpus],total_examples=1,epochs=10)\n",
    "skipgram_model.save(\"skipgram_model.model\")\n",
    "loaded_model = Word2Vec.load(\"skipgram_model.model\")\n",
    "vector_representation = loaded_model.wv['word']\n",
    "print(\"Vector representation of 'word':\",vector_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d58065",
   "metadata": {},
   "source": [
    "##### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b0faca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using GloVe: 0.802\n",
      "Similarity between 'india' and 'indian' using GloVe: 0.865\n",
      "Similarity between 'fame' and 'famous' using GloVe: 0.589\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "glove_model = load('glove-wiki-gigaword-50')\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "    similarity = glove_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fcbee",
   "metadata": {},
   "source": [
    "#### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5731c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using FastText: 0.642\n",
      "Similarity between 'india' and 'indian' using FastText: 0.708\n",
      "Similarity between 'fame' and 'famous' using FastText: 0.519\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "\n",
    "for pair in word_pairs:\n",
    "    similarity = fasttext_model.similarity(pair[0], pair[1])\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using FastText: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "796c728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amann\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/10.5 MB 4.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/10.5 MB 4.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/10.5 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.4/10.5 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/10.5 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.5 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.0/10.5 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.2/10.5 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.5/10.5 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.8/10.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.0/10.5 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.3/10.5 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.6/10.5 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.8/10.5 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.1/10.5 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.6/10.5 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.9/10.5 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.4/10.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.9/10.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.4/10.5 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.0/10.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.8/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.33.0 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\amann\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d9dfd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amann\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amann\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using BERT: 0.930\n",
      "Similarity between 'india' and 'indian' using BERT: 0.957\n",
      "Similarity between 'fame' and 'famous' using BERT: 0.956\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    tokens = tokenizer(pair,return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "    cls_embedding = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(cls_embedding[0],cls_embedding[1],dim=0)\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
