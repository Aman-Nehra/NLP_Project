{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20d90865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e7c40",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e9e14",
   "metadata": {},
   "source": [
    "##### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c1a836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello, how are you?\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa78f4",
   "metadata": {},
   "source": [
    "#### Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31103d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there.', 'How are you doing?', \"I hope you're well.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hello there. How are you doing? I hope you're well.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cd77a",
   "metadata": {},
   "source": [
    "#### Regexp Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e3998b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Email', 'me', 'at', 'test@example.com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "text = \"Email me at test@example.com\"\n",
    "tokens = regexp_tokenize(text,pattern=r'\\S+') # One or more non-white spaces\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78146d",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981244b",
   "metadata": {},
   "source": [
    "Stopwords are common words that carry little meaningful information, such as: \"is\", \"the\", \"and\", \"in\", \"to\" etc. They're often removed in text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9322eb9",
   "metadata": {},
   "source": [
    "#### Import and Download StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad32ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9486c",
   "metadata": {},
   "source": [
    "#### Get Stopwords List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0181e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hers', 'mightn', 'shan', 'than', 'nor', 'you', 'ourselves', \"couldn't\", 'did', 'above', 'why', 'other', 'because', 'once', 've', 'it', 'there', \"don't\", \"i'm\", 'between', 'its', 'yourself', 'more', 'does', 'or', 'that', 'itself', 'an', 'ma', \"i'll\", 'him', 'how', 'they', \"didn't\", 'those', 'can', 'on', \"that'll\", 'wouldn', 'own', \"she's\", 'he', 'his', \"we'd\", \"you'd\", 'needn', 'them', 'over', 'wasn', \"i've\", \"we've\", 'don', \"wouldn't\", 'were', 'was', 'being', 'out', 'through', 'while', 'only', 'herself', 'we', 'and', \"he'll\", 'doesn', 'having', 'but', 'your', 'won', 'didn', 'by', \"he'd\", \"should've\", 'whom', 'when', 'some', 'too', 'my', 'very', \"i'd\", 'has', 'both', 'be', \"they're\", \"you're\", \"hasn't\", 'this', 'any', 'again', 'doing', 'below', 'a', 'are', \"shan't\", 'of', \"hadn't\", \"they'd\", 'haven', 'about', 'yourselves', 'weren', \"aren't\", \"you'll\", \"won't\", 'before', 'i', \"mightn't\", 'theirs', \"it'd\", 'our', 'she', 'couldn', 'no', 'hasn', \"we're\", 'for', 'such', \"shouldn't\", 'll', \"mustn't\", 'am', 'up', 'at', 'have', \"weren't\", 'these', \"haven't\", 'hadn', 'as', 'here', \"he's\", 'same', 'shouldn', 'o', 'then', 'off', 'ours', \"they've\", 'who', 'their', 'do', 'm', \"isn't\", 'had', 'where', 'after', \"she'll\", 'd', 'to', 'until', 'further', \"it's\", \"it'll\", 'now', 'should', 'what', \"you've\", \"needn't\", 'all', 'the', 'with', 'themselves', \"we'll\", 'which', 'against', 'ain', \"they'll\", 'from', 'not', 's', 'mustn', 'so', 'will', 'few', 'himself', 'me', 'each', \"wasn't\", 'is', 'under', 'isn', 'been', 'myself', 'aren', 'if', 'y', \"she'd\", \"doesn't\", 'just', 'yours', 'her', 'down', 'during', 're', 'in', 't', 'into', 'most'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e7382",
   "metadata": {},
   "source": [
    "#### Remove Stopwords from a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72b7242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'showing', 'stop', 'word', 'filtering', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example showing off stop word filtering.\"\n",
    "words = word_tokenize(text)\n",
    "filtered = [word for word in words if word.lower() not in stop_words]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673a09b",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192347fc",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its base or root form (stem), usually by chopping off suffixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aac439",
   "metadata": {},
   "source": [
    "#### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90e10857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'player', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"playing\",\"played\",\"player\",\"plays\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738e21b",
   "metadata": {},
   "source": [
    "#### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e06841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'ran', 'run', 'runner']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "words = [\"running\",\"ran\",\"runs\",\"runner\"]\n",
    "stems = [snowball.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985fed3",
   "metadata": {},
   "source": [
    "#### Stemming with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "200c67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['play', 'play', 'play', 'player', 'jump']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Strip common suffixes like -ing, -ed , -s\n",
    "stemmer = RegexpStemmer(r'(ing|ed|s)$')\n",
    "words = ['playing', 'played', 'plays', 'player', 'jumps']\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772a037",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8da334",
   "metadata": {},
   "source": [
    "Lemmatization reduces a word to its base form (lemma), but uses vocabulary and grammar rules, so the result is a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b08f051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "run\n",
      "good\n",
      "well\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\"))    # → 'running' (default POS is noun)\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # → 'run' (verb base form of running is run)\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))   # → 'good'\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"r\")) # -> here better is used as an adverb 'r' is for adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fea74",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9808fbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7bf8d",
   "metadata": {},
   "source": [
    "## Named Entity Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b20292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  and/CC\n",
      "  became/VBD\n",
      "  the/DT\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "text = \"Barack Obama was born in Hawaii and became the President of the United States.\"\n",
    "# Step 1 : Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Step 2 : POS Tagging \n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Step 3 : Named Entity Chunking\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "print(named_entities)\n",
    "\n",
    "# Show GUI tree\n",
    "named_entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad47c0",
   "metadata": {},
   "source": [
    "## Spelling Correcting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1118bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"certain conditions during several generations are modified in the same manner\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "incorrect_text = \"ceertain conditionas duriing severl ggeneratoins aree moodified in the saame maner\"\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e97f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
